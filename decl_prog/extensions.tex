\chapter{Extensions}
\label{sec:extensions}

\section{Higher-order code}
\label{sec:ho}

In this section we show how higher-order code
can be embedded in first-order logic.
To do this, we need to define some additional abstract syntax,
define a suitable universe,
then provide a formal semantics.
Here we only cover higher-order predicates;
higher-order functions are handled in an analogous way.

Two additional pieces of abstract syntax are required:
lambda terms which create higher-order values,
and higher-order call formulas in which
a higher-order value is applied to arguments.
Lambda terms are written as follows:
\[ \lambda v_1 \ldots v_n. \phi \]
This stands for the abstraction of $\phi$
over the variables $v_1, \ldots, v_n$,
and corresponds to the Mercury expression:
\begin{verbatim}
    (pred(V1::Mode1, ..., VN::ModeN) is Detism :- Goal)
\end{verbatim}
If the arguments are not variables
then fresh ones are introduced and unifications moved to the body,
as is done with predicate completion.
Higher-order calls are written as follows:
\[
(t)(t_1, \ldots, t_n)
\]
This stands for a call to the higher-order term $t$
with $t_1, \ldots, t_n$ as arguments,
and corresponds to the Mercury goal \co{(t)(t1, ..., tN)}.

In order to extend our first-order universe
to handle higher-order terms,
we need to add elements that the lambda terms denote.
As with first-order predicates,
lambda terms denote relations over the universe---%
that is, mappings from tuples to truth values---%
with the key difference being that
these relations are also members of the universe itself.

For a first-order universe $U$,
the set of all $n$-ary relations corresponds to
the powerset of $U^n$.
We might try to construct a higher-order universe
by including the powerset along with the original set,
then including all terms
that can be constructed from what we would then have,
and so on.
We would, however, inevitably end up with a set
that supposedly includes its own powerset,
but this would violate the theorem of Cantor
that says this cannot happen.
In a sense,
the universe we have tried to define is too large to be considered a set.
We can repair this situation by limiting our notion of powerset
to only include \emph{computable} relations.

A higher-order model constructed by limiting the powerset relation
is known as a \emph{general model}.
If the powerset does not include every relation,
then it cannot properly characterize the intended interpretation
of second-order logic.
However, for the purposes of programming language semantics,
including just the computable relations
ought to be sufficient to cover
anything the programmer intends.
A full semantics for second-order logic
would be too powerful for our purposes.

With the universe we have just defined,
we can give our semantics.
Let $t$ be the lambda term $\lambda v_1 \ldots v_n. \phi$
and let $\sigma$ be an assignment.
Given $u_1, \ldots, u_n \in U$,
define $\sigma'$ as $\sigma \{ v_i \mapsto u_i \}$.
Then $I_\sigma(t)$ is the relation such that
$\langle u_1, \ldots, u_n \rangle$ maps to \true\ in $I_\sigma(t)$
if and only if $I_{\sigma'}(\phi)$ is true.

Conversely, let $t$ be any term denoting a higher-order value with arity $n$,
and let $\rho$ be an assignment.
Given terms $t_1, \ldots, t_n$,
we define $I_\rho((t)(t_1, \ldots, t_n))$ as true
if and only if the tuple
$\langle I_\rho(t_1), \ldots, I_\rho(t_n) \rangle$
maps to \true\ in the relation $I_\rho(t)$.

We can express the above two logical equivalences
more formally as follows:

\begin{center}
\begin{tabular}{rcl}
$\langle u_1, \ldots, u_n \rangle \mapsto \true \quad$ & &
\\
in $I_\sigma(\lambda v_1 \ldots v_n. \phi)$
& $\iff$ &
$I_{\sigma'}(\phi)$
where $\sigma' = \sigma \{ v_i \mapsto u_i \}$
\\
\\
$I_\rho((t)(t_1, \ldots, t_n))$
& $\iff$ &
$\langle I_\rho(t_1), \ldots, I_\rho(t_n) \rangle
\mapsto \true$ in $I_\rho(t)$
\\
\end{tabular}
\end{center}

\noindent
Free variables in the lambda expression,
that is, free variables in $\phi$ other than the $v_i$,
are assigned values by $\sigma$,
which is the assignment for the formula
where the lambda term occurs.
The $v_i$, on the other hand,
are assigned values by $\rho$,
which is the assignment for the formula
where the higher-order call occurs.

Lambda terms are implemented with ``closures\label{gi:closure}'',
which are ground data terms that consist of a code pointer,
along with a ground data term for each of the free variables
in the lambda term.
The ground data terms come from the substitution
at the point where the lambda term is constructed---%
in our operational semantics,
free variables in the lambda expression
have substitutions applied to them by earlier unifications,
in the same way that other free variables do.

The code pointer is to a piece of code generated by the compiler.
The generated code represents a predicate
whose arguments consist of the free variables in the lambda term,
followed by the $v_i$ variables.
Implementing the higher-order call involves
appending ground data terms for the higher-order call arguments
to the ground data terms in the closure,
then jumping to the code pointer
with these ground terms as the arguments.


\section{Partial functions}
\label{sec:partial}

We mentioned in Section~\ref{sec:sem-equality} that,
while the predicate calculus requires that all functions be total,
Mercury allows them to be partial in the form of \co{semidet} functions.
Classically, every term denotes something,
but for a \co{semidet} function applied to arguments
outside the function's domain
(that is, where the function application fails)
nothing is denoted.

Such terms are sometimes called non-denoting terms\label{gi:non-denoting},
and a logic that allows non-denoting terms is called a free logic.
We define the semantics by treating as false
any atomic formula containing a non-denoting term;
this approach is known as negative free logic (NFL)\label{gi:nfl}.

In NFL,
an ``existence check'' is required
for each partial function called within an atomic formula,
except those that are already of the form $y = \ct{f}(t_1, \ldots, t_n)$.
The existence check succeeds if and only if
the partial function term does actually denote something.

The existence check can be implemented by
equating the function call with a fresh variable,
and replacing the function call in the atomic formula
with the variable we have just introduced.
The original atomic formula is then replaced with
the conjunction of the new variable equation
and the new atomic formula,
with the new variable existentially quantified.

That is,
if $t$ is a partial function call occurring in the atomic formula $\phi$,
$v$ is a fresh variable,
and $\phi'$ is $\phi$ with the sub-term $t$ replaced by $v$,
then $\phi$ is replaced as follows:
\[
\phi \qquad \mathrm{becomes} \qquad \exists v.\, v = t \land \phi'
\]
If in $v = t$ the call to $t$ fails,
then $t$ is non-denoting,
so the conjunction fails
as NFL required the original atomic formula to do.
Thus,
since $\exists$ only quantifies over values in the universe,
existentially quantifying the fresh variable
quite literally performs the existence check.

For example, consider the following declaration
for a function that returns the $n$th element of a list,
or fails if $n$ is out of range.
\begin{verbatim}
    :- func index(list(T), int) = T is semidet.
\end{verbatim}
The goal
\begin{verbatim}
    p(index(L, 3))
\end{verbatim}
would be translated into
\begin{verbatim}
    some [V] ( V = index(L, 3), p(V) )
\end{verbatim}
If \co{L} has fewer than three elements
then \co{index(L, 3)} is non-denoting,
and the call to \co{p} should therefore fail.
And indeed the translated goal would,
since there is no value for \co{V}
for which the formula is true.

In our operational semantics
from Section~\ref{sec:resolution},
function calls are handled
in such a way as to be equivalent to the above.
The clause return value is used directly
instead of introducing the variable \co{V},
but in the end the effect is the same as
the translation we have just given,
since renamed variables from the clause
behave the same as existentially quantified variables.

NFL has some features often considered undesirable.
For example, the goal \co{index(L,N) = index(L,N)}
is false if the index is out of range,
even though syntactic identity suggests it ought to be true.
We have defined equality semantically, however,
in that two terms are equal if and only if they denote the same thing.
As such,
a non-denoting term can never be equal to another term,
including itself.

Perhaps more concerning is the effect on substitutivity.
Consider the following two goals:
\begin{verbatim}
    index(L, N) \= a

    V = index(L, N),
    V \= a
\end{verbatim}
Substitutivity would suggest that these are equivalent,
but in fact if the index is out of range
then the first succeeds but the second fails.
The explanation is that \verb#A \= B# is not an atomic formula,
it is an abbreviation for \co{not (A = B)}.
Any existence check for \co{A} or \co{B}
needs to be put inside the negation,
conjoined with the underlying atomic formula.
The proper translation is thus:
\begin{verbatim}
    not some [V] (V = index(L, N), V = a)
\end{verbatim}
As a consequence of this,
understanding the code requires knowing which goals are considered atomic,
and which look atomic but are actually abbreviations.

If any of these effects leave an unpleasant taste,
the best advice is to avoid \co{semidet} functions where possible
and use \co{semidet} predicates instead.
Functions are allowed to be \co{semidet}
because such functions are the natural way
to interpret field access functions
where there is more than one constructor in the type.
There is no obligation for users to write other functions in this way,
however,
and if there are such functions to be called,
they can always be wrapped in a \co{semidet} predicate.


\section{Exceptions}
\label{sec:exceptions}

\subsection{Catching}
\label{sec:catch}

Mercury allows for thrown exceptions to be caught
with the use of ``try'' goals.
One way a try goal can be written is as follows:
\begin{verbatim}
    try [] Goal
    then Then
    catch_any V -> Catch
\end{verbatim}
Here \co{Goal}, \co{Then} and \co{Catch} are goals,
and \co{V} is a variable.
We will assume that none of the goals can fail,
and neither can any of them succeed more than once.
(This is not the most general form of ``try'' goals;
see the Mercury reference manual for the full specification.)

Operationally,
\co{Goal} is executed first.
If that succeeds without throwing an exception
then \co{Then} is executed.
Otherwise, if an exception is thrown,
\co{Catch} is executed.
In the end the try goal as a whole succeeds exactly once.

The declarative semantics of this code is as follows:
\[
    (\phi_{\ct{goal}} \land \phi_{\ct{then}}) \lor
    (\exists v.\, \phi_{\ct{catch}})
\]
This states that either
$\phi_{\ct{goal}}$ and $\phi_{\ct{then}}$ are both true,
or there is some value $v$ (the exception value)
for which $\phi_{\ct{catch}}$ is true.
In other words,
there are multiple solutions to the try goal:
one from the normal code
and one from the exception-handling code
for each possible exception value.

If execution was to be operationally complete,
it would need to produce all of these solutions.
Obviously,
this is not what is intended:
we only want the solution from the normal code if possible,
otherwise a single solution from the exception-handling code
for the exception that is thrown.

For this we use the technique
that we introduced earlier in Section~\ref{sec:committed-choice}.
The compiler infers the try goal to be \co{cc\_multi},
and leaves an intentional completeness gap
between the declarative and operational semantics.
Thus,
while declaratively all solutions are defined,
operationally the try goal
is defined in terms of the operational semantics of $\phi_{\ct{goal}}$.
The behaviour is kept consistent with the declarative semantics,
since the compiler ensures that
backtracking will not require
both the normal and the exceptional outputs
to be produced.


\subsection{Throwing}
\label{sec:throw}

We have given a semantics for catching exceptions
that allows exception handling to be embedded
into the classical, two-valued logic.
How do we do the same with throwing exceptions?

It might be tempting to just say that, declaratively,
throwing an exception is the same as being false.
In both cases, no variable bindings are produced.
This does not work out, however,
since an exception thrown from inside a negation
should be the same as an exception thrown from outside.
If exceptions are meant to be false
then negated exceptions must be true,
which breaks our own rule.

The required operational requirements are sufficiently clear:
resolving an exception must neither succeed nor fail.
There can be no problem with soundness,
since success and failure are
the only results that would constrain the models.
This is not the case for completeness, however.
In order to maintain completeness,
throwing an exception cannot be considered valid
as that would require success,
and it cannot be considered unsatisfiable
as that would require failure.

There is a third option, however,
which is that throwing an exception
can be considered contingent\label{gi:contingent2}.
That is, there exists a model in which it is true
and another in which it is false.
With this arrangement
the deductive system can be seen to be reasonably complete,
since it would not be required to
prove anything at all in the case of thrown exceptions.

Another way of saying this is that, declaratively,
the throw predicate may be defined as follows.
\begin{verbatim}
    :- pred throw(T::in) is erroneous.
    throw(X) :- throw(X).
\end{verbatim}
It is thus declaratively equivalent to a loop.
Operationally, of course,
it immediately returns to the closest enclosing catch
rather than running forever.

One thing to be aware of when programming with exceptions
is that, in some cases,
the mode-determinism assertions imply
that a given call is equivalent to \true.
As mentioned in Section~\ref{sec:mode-det},
unless a strict operational semantics is used
the compiler may optimize away such calls
(the default semantics is strict,
so this does not happen unless
a non-default semantics is explicitly selected).
Thus, if the call is intended to throw an exception,
the exception may not end up being thrown.

The same issue can arise with the semantics we give here.
Consider the following goal:
\begin{verbatim}
    ( if throw(X) then Y = 1 else Y = 1 )
\end{verbatim}
Our semantics says this goal is equivalent to \co{Y = 1}
in any given model,
irrespective of whether or not $\ct{throw}(x)$
is true in that model.
The compiler would therefore be justified
in replacing this goal with \co{Y = 1},
meaning that the exception does not get thrown.

In general, however,
it is not possible to verify that the two branches
of the if-then-else goal have the same solutions,
except by executing both of them.
Thus, the operational semantics must still be incomplete,
since it is not always possible to do this.
This incompleteness does not matter, though,
since we can reasonably assume that
the programmer intends the exception to be thrown.


\section{Types}
\label{sec:types}

So far we have largely ignored types,
so the axioms we gave in Section~\ref{sec:axioms}
are technically incorrect.
However,
if we have unary predicates that correspond to each type,
we can adjust the quantifiers to account for types.
This process is known as \emph{relativization\label{gi:relativization}}.

For example,
given a type $T$
and a variable $x$ of this type,
we can quantify the variable in the formula $\phi$
by writing $\forall x\!:\!T.\, \phi$ and $\exists x\!:\!T.\, \phi$.
If the predicate corresponding to this type is $p_T$,
then these formulas can be treated as abbreviations for
$\forall x.\, p_T(x) \rightarrow \phi$ and
$\exists x.\, p_T(x) \land \phi$,
respectively.
This will ensure that only well-typed terms
will play a role in the semantics.
